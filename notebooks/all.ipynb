{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7d5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "import librosa as li\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the google cloud storage bucket we will be working on\n",
    "BUCKET_NAME = \"speech_emotion_bucket\"\n",
    "\n",
    "# the folder inside the bucket we will be working on\n",
    "PREFIX = \"Raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a client object to interact with GCS\n",
    "# loggin in into our GCS environment\n",
    "client = storage.Client()\n",
    "\n",
    "# the list of files (blobs) inside the bucket\n",
    "# stores those files into a variable blobs\n",
    "blobs = client.list_blobs(BUCKET_NAME, prefix=PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dictionary to store the waveform (y) of each audio file\n",
    "raw_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad6e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '“' (U+201C) (3658399629.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m“signal”: signal,\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '“' (U+201C)\n"
     ]
    }
   ],
   "source": [
    "# the google cloud storage bucket we will be working on\n",
    "BUCKET_NAME = \"speech_emotion_bucket\"\n",
    "\n",
    "# the folder inside the bucket we will be working on\n",
    "PREFIX = \"Raw/\"\n",
    "\n",
    "# creates a client object to interact with GCS\n",
    "# loggin in into our GCS environment\n",
    "client = storage.Client()\n",
    "\n",
    "# the list of files (blobs) inside the bucket\n",
    "# stores those files into a variable blobs\n",
    "blobs = client.list_blobs(BUCKET_NAME, prefix=PREFIX)\n",
    "\n",
    "# creates dictionary to store the waveform (y) of each audio file\n",
    "raw_data = {}\n",
    "\n",
    "# iterates trough each file inside the bucket\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith(\".wav\"):\n",
    "\n",
    "        # downloads each audio file and stores it into a binary variable\n",
    "        bytes = blob.download_as_bytes(raw_download=True)\n",
    "        binary = BytesIO(bytes)\n",
    "\n",
    "        # load the audio into librosa\n",
    "\t    # y is a NumPy array witht he waveform data\n",
    "        # sr is the sample rate (16khz)\n",
    "        y, sr = li.load(binary, sr=None)\n",
    "\n",
    "        # scales the waveform so the maximum value is 1\n",
    "        y = y / np.abs(y).max()\n",
    "\n",
    "        # remove silence\n",
    "        # silence is defined by a decibel threshold (tob_db)\n",
    "        # default should be around 60 db\n",
    "        y, _ = li.effects.trim(y)\n",
    "\n",
    "        # compute the mel spectogram\n",
    "        # it's a 2d array (rows = frequency ; columns = time ; values = intensity (brightness))\n",
    "        S = li.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "        # convert the spectogram from raw energy (power) into decibel scale (log scale)\n",
    "        S_dB = li.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # convert the standardized spectogram into an image file & save it in the cloud\n",
    "\n",
    "        # Stockage en mémoire\n",
    "        raw_data[blob.name] = {\n",
    "            “signal”: signal,\n",
    "            “sampling_rate”: sr\n",
    "        }\n",
    "        print(f”{blob.name} → {len(signal)} échantillons à {sr} Hz”)\n",
    "print(f”\\n:white_check_mark: {len(raw_data)} fichiers audio chargés dans `raw_data`.“)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_emotion_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
